---
title: "Illustration"
author: "Wenhao Gou | wg2364"
date: "4/17/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(extraDistr)
library(dplyr)
library(MASS)
library(LaplacesDemon)
```

# Re-state the question:

Climate researchers are interested in modeling the hurricane trajectories to forecast the wind speed. Let $t$ be time (in hours) since a hurricane began, and For each hurricane $i$, we denote $Y_{i}(t)$ be the wind speed of the $i$th hurricane at time $t$. The following Bayesian model was suggested.  


$$Y_{i}(t+6) =\beta_{0,i}+x_{i,1}\beta_{1,i} +
x_{i,2} \beta_{2,i} + x_{i,3}\beta_{3,i} +\beta_{4,i,j}Y_{i,j}(t) +  +\beta_{5,i,j}\Delta_{i,1}(t)+\beta_{6,i,j}\Delta_{i,2}(t)+ +\beta_{7,i}\Delta_{i,3} + \epsilon_{i}(t)$$   
where $x_{i,1}$ is the month of year when the hurricane started, $x_{i,2}$ is the calendar year of the hurricane, and $x_{i,3}$ is the type of hurricane, $\Delta_{i,1}(t)$, $\Delta_{i,2}(t)$ and $\Delta_{i,3}(t)$ is the change of latitude longitude, and wind speed between $t-6$ and $t$, and $\epsilon_{i,t}$ follows a  normal distributions with mean zero and variance $\sigma^2$, independent across $t$.


In the model,  $\boldsymbol{\beta}_{i} =  (\beta_{0,i},\beta_{1,i},...,\beta_{7,i})$ are the random coefficients associated the $i$th hurricane, we assume that 

$$\boldsymbol{\beta}_{i} \sim N(\boldsymbol{\beta}, \boldsymbol{\Sigma})$$
follows a multivariate normal distributions with mean $\boldsymbol{\beta}$ and covariance matrix $\Sigma$.


\paragraph{Prior distributions}


We assume the following non-informative or weak prior distributions for $\sigma^2$, $\boldsymbol{\beta}$ and $\Sigma$.
$$P(\sigma^2) \propto \frac{1}{\sigma^2};\quad P(\boldsymbol{\beta})\propto 1;\quad P(\Sigma^{-1}) \propto 
|\Sigma|^{-(d+1)} \exp(-\frac{1}{2}\Sigma^{-1})$$
d is dimension of $\beta$.


# Find out the posterial distribution:

It is easy to see that:

$$f(Y_{i}(t+6)|\boldsymbol{\beta}, \sigma) \sim N(\mu,\sigma^2)$$
Where $\mu$ is the linear combination of all coefficients.

Our objective is to derive $f(\boldsymbol{\beta}, \sigma|Y_{i}(t+6))$

\begin{align*}
f(\boldsymbol{\beta},\boldsymbol{\beta_i}, \boldsymbol{\Sigma}^{-1},\sigma^2|Y_{i}(t+6)) &\propto f(Y_{i}(t+6)|\boldsymbol{\beta_i}, \sigma^2) f(\boldsymbol{\beta_i}|\boldsymbol{\beta}, \boldsymbol{\Sigma}^{-1})f(\boldsymbol{\beta})f(\boldsymbol{\Sigma}^{-1})f(\sigma^2)\\
f(Y_{i}(t+6)|\boldsymbol{\beta_i}, \sigma)&\propto \prod_{i=1}^{N} \frac{1}{\sigma}\exp\{-\frac{(x_i - \mu_i)^2}{2\sigma^2}\} 
\;\; \text{,where } \mu_i = X^T_i\boldsymbol{\beta_i} \\
f(\boldsymbol{\beta_i}|\boldsymbol{\beta}, \boldsymbol{\Sigma}^{-1}) &\propto \frac{1}{\sqrt{|\boldsymbol{\Sigma^{-1}}|}} \exp \{ -\frac{1}{2} (\boldsymbol{\beta_i} - \boldsymbol{\beta})^T\boldsymbol{\Sigma}(\boldsymbol{\beta_i} - \boldsymbol{\beta})\} \\
f(\sigma^2) &\propto \frac{1}{\sigma^2}\\
f(\boldsymbol{\beta}) &\propto 1 \\
f(\boldsymbol{\Sigma}^{-1}) &\propto |\Sigma|^{-8} \exp(-\frac{1}{2}\Sigma^{-1})
\end{align*}

The full posterior, then, is simply the product of these three termsâ€”the likelihood, prior, and the distribution for $\Sigma$.
\begin{align*}
\prod_{i=1}^{N} \frac{1}{\sigma}\exp\{-\frac{(x_i - \mu_i)^2}{2\sigma^2}\} * \frac{1}{\sqrt{|\boldsymbol{\Sigma^{-1}}|}} \exp \{ -\frac{1}{2} (\boldsymbol{\beta_i} - \boldsymbol{\beta})^T\boldsymbol{\Sigma}(\boldsymbol{\beta_i} - \boldsymbol{\beta})\} * |\Sigma|^{-8} \exp(-\frac{1}{2}\Sigma^{-1})
\end{align*}


```{r}
data = read.csv("hurrican356.csv")
df = data %>%
  mutate(Month = factor(Month, levels = c("January", "April", "May", "June", "July", "August",
                                           "September", "October", "November", "December")),
         Month = as.numeric(Month),
         Nature = as.numeric(as.factor(Nature)))
id = df %>% group_by(Season, ID) %>% summarize(n = n())

upd.bi = function(df, epis, beta, sigma, n){
  sd = solve(epis + diag(n*sigma, 8, 8)) # posterior distribution for beta.i 
  mu = sd %*% (epis %*% beta + sigma*sum(df %*% beta)) # posterior distribution for beta.i
  return(list(sd = sd, mu = mu))}
mc = function(df, a, b, ini.sigma, niter = 2){
<<<<<<< HEAD
  d.lat = sapply(2:nrow(df), function(i){df[i,7] - df[i-1, 7]})
  d.lon = sapply(2:nrow(df), function(i){df[i,8] - df[i-1, 8]})
  d.spd = sapply(2:nrow(df), function(i){df[i,9] - df[i-1, 9]})
  d.yi = df[-1,9]
  dt = cbind(d.yi, df[-1,1], df[-1,4], df[-1,3], df[-1,5], df[-nrow(df),9], d.lat, d.lon, d.spd) 
  # final dataset for each individual grp
=======
  delta1 = c(0, sapply(2:nrow(df), function(i){df[i,7] - df[i-1, 7]}))
  delta2 = c(0, sapply(2:nrow(df), function(i){df[i,8] - df[i-1, 8]}))
  delta3 = c(0, sapply(2:nrow(df), function(i){df[i,9] - df[i-1, 9]}))
  dt = cbind(df[,1], df[,4], df[,3], df[,5], df[,9], delta1, delta2, delta3) # final dataset for each individual group
>>>>>>> 198e79a4f5745ebd7448378ee32e24267cc86359
  n = nrow(dt)
  epis = vector("list", niter)
  beta = matrix(NA, nrow = 8, ncol = niter)
  beta.i = matrix(NA, nrow = 8, ncol = niter)
  sigma = rep(NA, niter)
  epis[[1]] = cor(matrix(1/extraDistr::rinvgamma(64, alpha = a, beta = b), 8, 8)) 
  # initial epsilon^-1 ~ inverse.gamma(0.001, 0.001) (8x8)
  beta[,1] = rep(1, 8) # initial beta (8x1)
  sigma[1] = ini.sigma # initial sigma 
  beta.i[,1] = mvrnorm(1, mu = upd.bi(dt, epis[[1]], beta[,1], sigma[1], n)$mu, 
  Sigma = upd.bi(dt, epis[[1]], beta[,1], sigma[1], n)$sd) 
  # mvrnorm generates the beta_i for each subgroup(8x1) - beta.i ~ N(beta, epsilon)
  for (i in 2:niter){
    epis[[i]] = rinvwishart(n + a, diag(sum((beta.i[,i-1] - beta[,i-1])^2), 8, 8) + 0.001) 
    # use beta.i and beta update epsilon, pi(epsilon) ~ Inviwishart(n + a, beta_variance + b)!!!!!!!!(May change back to inverse gamma?????)
    beta[,i] = mvrnorm(1, mu = n*beta.i[,i-1], Sigma = n*epis[[i]])  # beta ~ N(n*beta_i, n*epsilon)
    sigma[i] = extraDistr::rinvgamma(1, alpha = n + 1, sum((dt[,5] - dt %*% beta.i[,i-1])^2)) 
    # sigma ~ inverse.gamma(n + 1, sample residual of Yi)
    beta.i[,i] = mvrnorm(1, mu = upd.bi(dt, epis[[i]], beta[,i], sigma[i], n)$mu, 
  Sigma = upd.bi(dt, epis[[i]], beta[,i], sigma[i], n)$sd)}
  return(list(beta.i =  beta.i))}

grp = function(df, id){
  sub.grp = vector("list", nrow(id))
  id.n = c(0, id$n)
  for (i in 1:length(sub.grp)){
    sub.grp[[i]] = df[(id.n[i]+1):id.n[i+1],]}
  return(sub.grp)}
 
```

Posterior Distribution Inference:

$Y_{it} = X_{i}\beta_{i} + \epsilon_{it}$, with $i = 1, 2, ....,n,\ t = 1, 2, .... t_i$
1. $\beta_i|.$:
$$Y_{it} \sim N(X_i\beta_i, \sigma^2),\ \beta_i\sim N(\beta, \Sigma^{-1})$$
$$\pi(Y_{it}|X_i\beta_i, \sigma^2) \propto \prod^n_{i=1}\prod^{t_i}_{t=1} \sigma^{-2}exp\{{-(\frac{Y_{it} - X_{i}\beta_{i})^2}{2\sigma^2}\}}$$
$$\pi(X_i\beta_i | Y_{it}, \sigma^2) \propto \pi(Y_{it}|X_i\beta_i, \sigma^2) \cdot \pi(\beta_i)$$
$$\propto \prod^n_{i=1}\prod^{t_i}_{t=1} \sigma^{-1}\Sigma^{-1/2}exp\{{-\frac{1}{2}(Y_{it} - X_{i}\beta_{i})^T(I\sigma_i^{-2})(Y_{it} - X_{i}\beta_{i})\}} \cdot exp{\{-\frac{1}{2}(\beta_i - \beta)^T\Sigma^{-1}(\beta_i - \beta)\}}$$
$$\propto exp{\{-\frac{1}{2}((\sigma_i^2(X_i^TX_i)^{-1}) + \Sigma^{-1})^{-1}\sum^n_{i=1}(\Sigma^{-1}\beta + I\sigma_i^{-2}X_i^TY_{it}))^T((\sigma_i^2(X_i^TX_i)^{-1} + \Sigma^{-1})(\Sigma^{-1}\beta + I\sigma_i^{-2}X_i^TY_{it}))\}}$$
$$\beta_i \sim N(K(\Sigma^{-1}\beta + \sigma^{-2}X_i^TY_{it}), K), where\ K = (\Sigma^{-1} + (\sigma^{-2}X_i^TX_i)^{-1})^{-1}$$
2.$\sigma^2|.$:
$$\pi(\sigma_i^2|.) = \pi(Y_{it}|X_i\beta_i) \cdot \pi(\sigma_i^2)$$
$$\propto \prod^n_{i=1}\prod^{t_i}_{t=1} \frac{1}{\sqrt{\sigma^2}}exp\{{-\frac{1}{2}(Y_{it} - X_{i}\beta_{i})^T(I\sigma_i^{-2})(Y_{it} - X_{i}\beta_{i})\}} \cdot \frac{1}{\sigma^2}$$
$$\propto \sigma^{-(n-2)}\sum^n_{i=1}\sum^{ti}_{t=1}exp{\{-\frac{1}{2}(Y_{it} - X_i\beta_i)^T(Y_{it}-Xi\beta_i)\sigma^{-2}\}}$$
$$\sigma^2 \sim InvGamma(\frac{n}{2}, SSR),\ where\ SSR = \sum^n_{i=1}\sum^{ti}_{t=1}(Y_{it} - X_i\beta_i)^T(Y_{it}- X_i\beta_i)$$

3. $\Sigma^{-1}|.$:
$$\pi(\Sigma^{})$$




